{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del Lenguaje Natural\n",
    "\n",
    "## Generación de un modelo de lenguaje utilizando redes neuronales\n",
    "\n",
    "Francisco Pablo Rodrigo\n",
    "\n",
    "El modelo del lenguaje neuronal propuesto por Bengio (2003) es un modelo que estima probabilidades a partir de una red neuronal FeddForward. Como otros modelos, se puede entender como una tupla:\n",
    "\n",
    "$$\\mu = (\\Sigma, P)$$\n",
    "\n",
    "donde $\\Sigma$ es el vocabulario de palabras y $P = p(w_j|w_i)$ es la probabilidad de transición de $w_i$ a $w_j$. En este caso $P$ es una red FeedForward con una arquitectura constituida por:\n",
    "\n",
    "Una capa de embedding.\n",
    "Una capa oculta con activación $\\tanh$.\n",
    "Una capa de salida con activación Softmax para obtener las probabilidades de transición.\n",
    "A continuación mostramos una aplicación del modelo. No mostramos el modelo que en este caso es un script (LM_bengio). Por tanto, importamos este script y la paqueteria necesaria.\n",
    "\n",
    "### Configuraciones previas\n",
    "\n",
    "Se realizan los *imports* necesarios para la creación del modelo. Para el entrenamiento del modelo se puede hacer uso de PyTorch o Tensorflow o cualquier otra librería que tenga redes neuronales pre-entrenadas, sin embargo, para este ejercicio se creará la red desde cero utilizando simplemente _numpy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos tres funciones:\n",
    "\n",
    "* Función para crear el vocabulario: asocia índices numéricos a palabras\n",
    "* Función para asociar a cada elemento, una palabra\n",
    "* Función para visualizar los embeddings por reducción de dimensionalidad con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que crea un vocabulario de palabras con un indice numero\n",
    "def vocab():\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len (vocab)\n",
    "    return vocab\n",
    "\n",
    "# Funcion que pasa la cadena de simbolos a una secuencia con indices numericos\n",
    "def text2numba(corpus, vocab):\n",
    "    for doc in corpus:\n",
    "        yield [vocab[w] for w in doc]\n",
    "        \n",
    "#Función para visualizar los embeddings\n",
    "#Usa reducción de la dimensionalidad por PCA\n",
    "def plot_words(Z,ids):\n",
    "    Z = PCA(2).fit_transform(Z)\n",
    "    r=0\n",
    "    plt.scatter(Z[:,0],Z[:,1], marker='o', c='blue')\n",
    "    for label,x,y in zip(ids, Z[:,0], Z[:,1]):\n",
    "        plt.annotate(label, xy=(x,y), xytext=(-1,1), textcoords='offset points', ha='center', va='bottom')\n",
    "        r+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elección de un corpus\n",
    "\n",
    "Un **corpus** es una muestra bien organizada del nuestro lenguaje tomada de materiales escritos o hablados y que se encuentran agrupados bajo un críterio común.Para esta práctica se utilizará un corpus en *español*.\n",
    "\n",
    "Obtenemos las sentencias con las que vamos a trabajar. Tokenizamos por oraciones y cada oración, a su vez, es tokenizada por palabras para obtener los elementos que servirán para el modelo del lenguaje.\n",
    "\n",
    "Posteriormente, separamos los datos del corpus en el corpus de entrenamiento y el de evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de cadenas train: 88\n",
      "Número de cadenas test: 39\n"
     ]
    }
   ],
   "source": [
    "# OPCION 1\n",
    "sents =  [word_tokenize(s) for s in sent_tokenize(open('corpus/funes_el_memorioso.txt','r').read())]\n",
    "\n",
    "#Split en corpus train y test\n",
    "corpus, corpus_eval = train_test_split(sents, test_size=0.3)\n",
    "\n",
    "print('Número de cadenas train:',len(corpus))\n",
    "print('Número de cadenas test:',len(corpus_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCION 2\n",
    "#import nltk\n",
    "#nltk.download('gutenberg')\n",
    "#sents = cess_esp.sents()\n",
    "#nltk.download('punkt')\n",
    "#sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver el número de tipos y tokens con el que cuenta el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tipos: 856 \n",
      "Número de tokens: 2229\n"
     ]
    }
   ],
   "source": [
    "#Frecuencia de los tipos\n",
    "freq_words= Counter( chain(*[' '.join(sent).lower().split() for sent in corpus]) )\n",
    "\n",
    "print('Número de tipos: {} \\nNúmero de tokens: {}'.format(len(freq_words), sum(freq_words.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sustitución de los hapax\n",
    "\n",
    "Ahora sustituiremos elementos del texto por el símbolo de fuera del vocabulario (Out Of Vocabulary) o $OOV$ esto nos permitirá manejar elementos que no se observen durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuevo corpus remplazando hápax por OOV\n",
    "corpus_hapax = []\n",
    "#Reemplazamos los hápax por OOV\n",
    "for sent in corpus:\n",
    "  sent_hapax =[]\n",
    "  for w in sent:\n",
    "    #Si es hápax\n",
    "    if freq_words[w.lower()] == 1:\n",
    "      #Se reemplaza por <oov>\n",
    "      sent_hapax.append('<oov>')\n",
    "    else:\n",
    "      #De otra forma se mantiene la palabra en mínuscula\n",
    "      sent_hapax.append(w.lower())\n",
    "  #Se agrupan las cadenas    \n",
    "  corpus_hapax.append(sent_hapax)\n",
    "    \n",
    "#print(corpus_hapax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Stemming\n",
    "\n",
    "Para esta tarea no realiza el procesos de steamming con la finalidad de simplificar la validación del modelo, ya que de otra manera se deberían reconstruir las cadenas a la hora de evaluar el modelo o a la hora de usarlo para alguna aplicación, por ejemplo, la generación de oraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Insertar símbolos de inicio y final de cadena\n",
    "\n",
    "Se indexa cada simbolo del vocabulario previamente tratado (sin _stopwords_ y _estemmizado_) para tener un modelo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion para crear un vocabulario\n",
    "idx = vocab() # Simplemente se renombra la funcion\n",
    "\n",
    "cads_idx = list(text2numba(corpus_hapax,idx))\n",
    "\n",
    "#print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se colocarán etiquetas al inicio y al final de cada sentencia: BOS (Beginning of Sentence) y EOS (End of Sentence) respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194, 0, 1, 2, 1, 3, 4, 5, 6, 1, 3, 7, 8, 9, 10, 1, 1, 2, 11, 12, 13, 14, 1, 3, 1, 9, 7, 1, 3, 15, 16, 1, 2, 17, 18, 19, 20, 7, 1, 9, 21, 1, 22, 1, 1, 3, 15, 1, 11, 1, 4, 7, 1, 1, 9, 17, 1, 22, 1, 3, 1, 9, 11, 1, 22, 1, 3, 15, 1, 23, 1, 9, 1, 24, 1, 25, 1, 26, 27, 28, 29, 193], [194, 24, 1, 30, 1, 31, 1, 9, 15, 1, 8, 1, 29, 193]]\n"
     ]
    }
   ],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "# A cada etiqueta se le asigna el indice número mayor \n",
    "# que el último indice asignado al vocabulario\n",
    "\n",
    "BOS_IDX = max(idx.values()) + 2\n",
    "EOS_IDX = max(idx.values()) + 1\n",
    "\n",
    "# Se agregan las etiquetas al vocabulario\n",
    "idx[EOS] = EOS_IDX\n",
    "idx[BOS] = BOS_IDX\n",
    "\n",
    "# Agregamos las etiquetas BOS al inicio y EOS al final de cada sentencia\n",
    "\n",
    "strings = [[BOS_IDX] + cad + [EOS_IDX] for cad in cads_idx]\n",
    "\n",
    "print(strings[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bigramas\n",
    "\n",
    "Antes de entrenar el modelo del lenguaje obtendremos los pares de entrenamiento que serán los pares obtenidos de bigramas, de tal forma que nuestro conjunto supervisado será:\n",
    "\n",
    "$$\\mathcal{S} = \\{(i,j) : (w_i, w_j) \\text{ es un bigrama}\\}$$\n",
    "Antes de obtener estos bigramas, además, debemos agregar los símbolos de $BOS$ y $EOS$, así como crear el vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317\n"
     ]
    }
   ],
   "source": [
    "# Creacion de bigramas\n",
    "bigrams = list(chain(*[zip(cad,cad[1:]) for cad in strings]))\n",
    "print(len(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [01:01<00:54,  2.70s/it]"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "iterations = 50\n",
    "eta = 0.1\n",
    "\n",
    "# dim -> hiperparametro que define la dimensioón de los vectores-palabra\n",
    "dim = 100\n",
    "m = 300\n",
    "N = len(idx)\n",
    "\n",
    "# Se usa para generar vectores one-shot\n",
    "matrix_I = np.identity(N)\n",
    "\n",
    "# Embebidding\n",
    "C = np.random.randn(dim,N) / np.sqrt(N)\n",
    "\n",
    "# Oculta\n",
    "W = np.random.randn(m,dim) / np.sqrt(dim)\n",
    "b = np.ones(m)\n",
    "\n",
    "# Salida\n",
    "U = np.random.randn(N,m) / np.sqrt(m)\n",
    "c = np.ones(N)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,iterations)):    \n",
    "    for bigram in bigrams:\n",
    "        \n",
    "        # FOWARD       \n",
    "        \n",
    "        # Capa embbeding\n",
    "        c_i = C.T[bigram[0]]\n",
    "        \n",
    "        # Capa oculta\n",
    "        h_i = np.tanh(np.dot(W,c_i) + b)\n",
    "        \n",
    "        # Pre-activacion\n",
    "        a = np.dot(U,h_i) + c\n",
    "        \n",
    "        # Salidas\n",
    "        tmp = np.exp(a - np.max(a))\n",
    "        # Aplicando softmax\n",
    "        f = tmp/tmp.sum(0)\n",
    "        \n",
    "        # BACKPROPAGATION para salida\n",
    "        d_out = f\n",
    "        k= bigram[1]\n",
    "        d_out[k] -= 1\n",
    "     \n",
    "        # Backpropagation para la capa oculta\n",
    "        dh = (1-h_i**2)*np.dot(U.T,d_out) \n",
    "        \n",
    "        # Backpropagation para la capa embedding\n",
    "        dc = np.dot(W.T,dh)\n",
    "        c -= eta*d_out\n",
    "\n",
    "        # Actualizacion de la capa de salida\n",
    "        U -= eta*np.outer(d_out,h_i)\n",
    "        \n",
    "        # Actualizacion de capa oculta\n",
    "        W -= eta*np.outer(dh,c_i)\n",
    "        b -=eta*dh\n",
    "        \n",
    "        # Actualizacion embedding\n",
    "        C -= eta*np.outer(dc,matrix_I[bigram[0]].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluación del modelo\n",
    "\n",
    "Entrenada la red, definimos una función forward para obtener las probabilidades a partir de la red ya entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    # Capa embbeding\n",
    "    c_i = C.T[x]\n",
    "    # Capa oculta\n",
    "    h_i = np.tanh(np.dot(W,c_i) + b)\n",
    "    # Pre-activacion\n",
    "    a = np.dot(U,h_i) + c\n",
    "    # Salidas\n",
    "    tmp = np.exp(a - np.max(a))\n",
    "    # Aplicando softmax\n",
    "    f = tmp/tmp.sum(0)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "\n",
    "for word in idx.keys():\n",
    "    lista.append((word,forward(idx['recuerdo'])[idx[word]]))\n",
    "    #print(word,forward(idx['presidente'])[idx[word]])\n",
    "\n",
    "lista.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "lista[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [w[0] for w in sorted(idx.items(), key=itemgetter(1))]\n",
    "plot_words(C.T[1:20],label[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evalaur el modelo, necesitamos primero definir una función que nos de la probabilidad de las cadenas. Definimos esta función a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_sent(sent):\n",
    "    #Obtenemos los simbolos\n",
    "    seq = sent.split()\n",
    "    #Obtenemos los bigramas de la cadena de evaluacion\n",
    "    bigrSeq = zip(seq,seq[1:])\n",
    "    \n",
    "    #Guardamos la probabilidad inicial dado el modelo\n",
    "    try:\n",
    "        p = forward(idx['<BOS>'])[idx[seq[0]]]\n",
    "    except: \n",
    "        p = forward(idx['<BOS>'])[idx['<oov>']]\n",
    "    #Multiplicamos por las probabilidades de los bigramas dado el modelo\n",
    "    for gram1, gram2 in bigrSeq:\n",
    "        #Obtiene las probabilidades de transición\n",
    "        #Dado el primer elemento\n",
    "        try:\n",
    "            prev_prob = forward(idx[gram1])\n",
    "        #En caso de que sea una OOV\n",
    "        except:\n",
    "            prev_prob = forward(idx['<oov>'])\n",
    "        #Obtiene la probabilidad de transitar a la siguiente palabra\n",
    "        try:\n",
    "            p *= prev_prob[idx[gram2]]\n",
    "        #En caso de que sea una OOV\n",
    "        except:\n",
    "            p *= prev_prob[idx['<oov>']]\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sent('los ojos cerrados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con esto, podemos evaluar el modelo con entropía empírica (tomamos el promedio por cadena de ésta). Asimismo, con base en la entropía empírica podemos obtener la perplejidad como: \n",
    "$$Px(\\mu) = 2^{H(\\mu)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación del modelo\n",
    "H = 0.0\n",
    "for cad in corpus_eval:\n",
    "    #Probabilidad de la cadena\n",
    "    p_cad = prob_sent(' '.join(cad))\n",
    "    #Longitud de la cadena\n",
    "    M = len(cad)\n",
    "    #Obtenemos la entropía cruzada de la cadena\n",
    "    if p_cad == 0:\n",
    "        pass\n",
    "    else:\n",
    "        H -= (1./M)*(np.log(p_cad)/np.log(2))\n",
    "        \n",
    "H = H/len(corpus_eval)\n",
    "\n",
    "print('Entropía promedio: {}\\nPerplejidad total: {}'.format(H,2**H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Calcular la proabilidad de 5 oraciones no vistas en el entrenamiento.\n",
    "# 7. Guardar los vectores de la capa de embedding asociados a las palabras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
